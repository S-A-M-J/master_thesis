{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f1da7c",
   "metadata": {},
   "source": [
    "# Cave Exploration RL Training\n",
    "\n",
    "This notebook contains the cave exploration reinforcement learning training pipeline, organized into separate cells for better modularity and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7152252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING IMPORTS CELL ===\n",
      "JAX backend info:\n",
      "JAX backend info:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2198632/2628047245.py:26: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
      "  print(f\"Platform: {xla_bridge.get_backend().platform}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: gpu\n",
      "Device count: 2\n",
      "Devices: [CudaDevice(id=0), CudaDevice(id=1)]\n",
      "GPU available: True\n",
      "GPU device: cuda:0\n",
      "Basic imports completed...\n",
      "Brax imports completed...\n",
      "Task-specific imports completed...\n",
      "=== ALL IMPORTS LOADED SUCCESSFULLY! ===\n",
      "Brax imports completed...\n",
      "Task-specific imports completed...\n",
      "=== ALL IMPORTS LOADED SUCCESSFULLY! ===\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports and setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add execution tracking to debug duplicate output\n",
    "print(\"=== STARTING IMPORTS CELL ===\")\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "from jax.lib import xla_bridge\n",
    "\n",
    "print(\"JAX backend info:\")\n",
    "print(f\"Platform: {xla_bridge.get_backend().platform}\")\n",
    "print(f\"Device count: {xla_bridge.get_backend().device_count()}\")\n",
    "print(f\"Devices: {xla_bridge.get_backend().devices()}\")\n",
    "\n",
    "# JAX configuration for numerical stability\n",
    "jax.config.update('jax_default_matmul_precision', 'high')\n",
    "jax.config.update('jax_traceback_filtering', 'off')\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_available = jax.devices()[0].platform == 'gpu'\n",
    "print(f\"GPU available: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    gpu_device = jax.devices('gpu')[0]\n",
    "    print(f\"GPU device: {gpu_device}\")\n",
    "else:\n",
    "    print(\"No GPU device found.\")\n",
    "\n",
    "import signal\n",
    "import json\n",
    "import functools\n",
    "import mujoco\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "import gc\n",
    "\n",
    "print(\"Basic imports completed...\")\n",
    "\n",
    "# Brax and training imports\n",
    "from brax.io import model\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from flax.training import orbax_utils\n",
    "from orbax import checkpoint as ocp\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from mujoco_playground import wrapper\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "print(\"Brax imports completed...\")\n",
    "\n",
    "# Task-specific imports\n",
    "from tasks.cave_exploration.cave_exploration import CaveExplore\n",
    "from tasks.common.randomize import domain_randomize as reachbot_randomize\n",
    "from utils.telegram_messenger import send_message_sync\n",
    "\n",
    "print(\"Task-specific imports completed...\")\n",
    "\n",
    "# Global variables\n",
    "ENV_STR = 'Go1JoystickFlatTerrain'\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "# Signal handler for graceful interruption\n",
    "def signal_handler(sig, frame):\n",
    "    print('Program exited via keyboard interrupt')\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# JSON encoder for JAX arrays\n",
    "class JaxArrayEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, jp.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "print(\"=== ALL IMPORTS LOADED SUCCESSFULLY! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9657d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configuration completed!\n",
      "Simulation dt: 0.004\n",
      "Distance to target reward scale: 70.0\n"
     ]
    }
   ],
   "source": [
    "# Environment configuration\n",
    "from tasks.cave_exploration.cave_exploration import default_config as reachbot_config\n",
    "\n",
    "env_cfg = reachbot_config()\n",
    "\n",
    "# Basic simulation parameters\n",
    "env_cfg.sim_dt = 0.004\n",
    "env_cfg.action_scale = 1\n",
    "\n",
    "# PID control parameters\n",
    "env_cfg.Kp_pri = 60.0\n",
    "env_cfg.Kd_pri = 20.0\n",
    "env_cfg.Kp_rot = 25.0\n",
    "env_cfg.Kd_rot = 2.0\n",
    "\n",
    "# Reward scaling configuration\n",
    "env_cfg.reward_config.scales.orientation = -0.0\n",
    "env_cfg.reward_config.scales.lin_vel_z = -0.0\n",
    "env_cfg.reward_config.scales.ang_vel_xy = -0.00\n",
    "env_cfg.reward_config.scales.torques = -0.00005\n",
    "env_cfg.reward_config.scales.action_rate = -0.0001\n",
    "env_cfg.reward_config.scales.dof_pos_limits = -0.5\n",
    "env_cfg.reward_config.scales.energy = -0.00001\n",
    "env_cfg.reward_config.scales.feet_slip = -0.0\n",
    "\n",
    "# Target-based rewards\n",
    "env_cfg.reward_config.scales.distance_to_target = 10.0\n",
    "env_cfg.reward_config.scales.vel_to_target = 3.0\n",
    "env_cfg.reward_config.scales.exploration_rate = 0.0\n",
    "\n",
    "print(\"Environment configuration completed!\")\n",
    "print(f\"Simulation dt: {env_cfg.sim_dt}\")\n",
    "print(f\"Distance to target reward scale: {env_cfg.reward_config.scales.distance_to_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO training parameters:\n",
      "  action_repeat: 1\n",
      "  batch_size: 512\n",
      "  discounting: 0.97\n",
      "  entropy_cost: 0.02\n",
      "  episode_length: 10000\n",
      "  learning_rate: 0.0003\n",
      "  max_grad_norm: 1.0\n",
      "  network_factory: policy_hidden_layer_sizes: &id001 !!python/tuple\n",
      "- 512\n",
      "- 256\n",
      "- 128\n",
      "policy_obs_key: state\n",
      "value_hidden_layer_sizes: *id001\n",
      "value_obs_key: privileged_state\n",
      "\n",
      "  normalize_observations: True\n",
      "  num_envs: 4096\n",
      "  num_evals: 10\n",
      "  num_minibatches: 16\n",
      "  num_resets_per_eval: 1\n",
      "  num_timesteps: 60000000\n",
      "  num_updates_per_batch: 8\n",
      "  reward_scaling: 1.0\n",
      "  unroll_length: 100\n",
      "\n",
      "PPO parameters configuration completed!\n"
     ]
    }
   ],
   "source": [
    "# PPO training parameters configuration\n",
    "ppo_params = locomotion_params.brax_ppo_config(ENV_STR)\n",
    "ppo_training_params = dict(ppo_params)\n",
    "\n",
    "# Modify params for training\n",
    "ppo_training_params[\"num_timesteps\"] = 30_000_000\n",
    "ppo_training_params[\"episode_length\"] = 10000\n",
    "ppo_training_params[\"num_envs\"] = 4096\n",
    "ppo_training_params[\"batch_size\"] = 1024\n",
    "ppo_training_params[\"num_minibatches\"] = 32\n",
    "ppo_training_params[\"num_updates_per_batch\"] = 16\n",
    "ppo_training_params[\"unroll_length\"] = 200\n",
    "ppo_training_params[\"entropy_cost\"] = 0.02\n",
    "ppo_training_params[\"learning_rate\"] = 0.0003\n",
    "\n",
    "# Set episode length in environment config\n",
    "env_cfg.episode_length = ppo_training_params[\"episode_length\"]\n",
    "\n",
    "print(\"PPO training parameters:\")\n",
    "for key, value in ppo_training_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nPPO parameters configuration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab8c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ PID: 2198632 | Thread: 139864404193920 | Time: 2025-07-05 18:20:14.578427\n",
      "Found 3 cave folders in /home/ga53voq/master_thesis/tasks/cave_exploration/environment/caves.\n",
      "Loading 1 cave environments.\n",
      "initial_qpos (cave_batch_loader): [-0.234  0.156  0.009  0.339 -0.683 -0.587 -0.283  0.003 -0.    -0.\n",
      "  0.    -0.    -0.    -0.003 -0.    -0.     0.004 -0.001  0.021]\n",
      "Floor boxes detected: 4404\n",
      "initial_qpos (cave_batch_loader): [-0.234  0.156  0.009  0.339 -0.683 -0.587 -0.283  0.003 -0.    -0.\n",
      "  0.    -0.    -0.    -0.003 -0.    -0.     0.004 -0.001  0.021]\n",
      "Floor boxes detected: 4404\n",
      "Found 4 boom end geoms\n",
      "Found 4405 floor/wall geoms\n",
      "CaveExplore task initialized with model: ReachbotModelType.BASIC\n",
      "CaveExplore task action space: 12\n",
      "Found 4 boom end geoms\n",
      "Found 4405 floor/wall geoms\n",
      "CaveExplore task initialized with model: ReachbotModelType.BASIC\n",
      "CaveExplore task action space: 12\n",
      "CaveExplore task observation space: {'privileged_state': (134,), 'state': (75,)}\n",
      "CaveExplore task observation space: {'privileged_state': (134,), 'state': (75,)}\n",
      "Saving configs\n",
      "Configuration saved to /home/ga53voq/master_thesis/tasks/cave_exploration/logs/cave_exploration-2025-07-05_18-20-14/config.json\n",
      "Training the model...\n",
      "Saving configs\n",
      "Configuration saved to /home/ga53voq/master_thesis/tasks/cave_exploration/logs/cave_exploration-2025-07-05_18-20-14/config.json\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ga53voq/.conda/envs/pyenv/lib/python3.12/site-packages/jax/_src/interpreters/xla.py:119: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max contacts updated to 500 based on current step.\n",
      "step: 0/60000000 (0.0%), reward: 0.174 +/- 0.232, time passed (min): 7.91 min, calculated time left (min): unknown min\n",
      "step: 0/60000000 (0.0%), reward: 0.174 +/- 0.232, time passed (min): 7.91 min, calculated time left (min): unknown min\n",
      "step: 7372800/60000000 (12.3%), reward: 1.319 +/- 1.154, time passed (min): 22.93 min, calculated time left (min): 163.68 min\n",
      "step: 7372800/60000000 (12.3%), reward: 1.319 +/- 1.154, time passed (min): 22.93 min, calculated time left (min): 163.68 min\n",
      "step: 14745600/60000000 (24.6%), reward: 2.416 +/- 2.321, time passed (min): 36.60 min, calculated time left (min): 112.33 min\n",
      "step: 14745600/60000000 (24.6%), reward: 2.416 +/- 2.321, time passed (min): 36.60 min, calculated time left (min): 112.33 min\n",
      "step: 22118400/60000000 (36.9%), reward: 4.609 +/- 3.530, time passed (min): 50.27 min, calculated time left (min): 86.09 min\n",
      "step: 22118400/60000000 (36.9%), reward: 4.609 +/- 3.530, time passed (min): 50.27 min, calculated time left (min): 86.09 min\n",
      "step: 29491200/60000000 (49.2%), reward: 5.546 +/- 3.611, time passed (min): 63.93 min, calculated time left (min): 66.13 min\n",
      "step: 29491200/60000000 (49.2%), reward: 5.546 +/- 3.611, time passed (min): 63.93 min, calculated time left (min): 66.13 min\n",
      "step: 36864000/60000000 (61.4%), reward: 7.054 +/- 3.705, time passed (min): 77.60 min, calculated time left (min): 48.70 min\n",
      "step: 44236800/60000000 (73.7%), reward: 7.188 +/- 4.292, time passed (min): 91.29 min, calculated time left (min): 32.53 min\n",
      "step: 51609600/60000000 (86.0%), reward: 9.896 +/- 5.522, time passed (min): 104.97 min, calculated time left (min): 17.06 min\n",
      "step: 58982400/60000000 (98.3%), reward: 11.592 +/- 7.317, time passed (min): 118.68 min, calculated time left (min): 2.05 min\n",
      "step: 66355200/60000000 (110.6%), reward: 12.860 +/- 9.270, time passed (min): 132.40 min, calculated time left (min): -12.68 min\n",
      "Training completed successfully!\n",
      "time to jit: 0:07:54.699525\n",
      "time to train: 2:04:29.000891\n",
      "Training completed! Results saved to: /home/ga53voq/master_thesis/tasks/cave_exploration/logs/cave_exploration-2025-07-05_18-20-14\n",
      "Final reward: 12.860 Â± 9.270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "print(f\"ðŸ”¥ PID: {os.getpid()} | Thread: {threading.current_thread().ident} | Time: {datetime.now()}\")\n",
    "\n",
    "# Training execution\n",
    "# Create log directory for training run\n",
    "datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logdir = os.path.join(os.getcwd(), \"logs/cave_exploration-\"+datetime_str)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Create environment\n",
    "env = CaveExplore(config=env_cfg)\n",
    "\n",
    "# Initialize tracking variables\n",
    "timesteps = []\n",
    "rewards = []\n",
    "total_rewards = []\n",
    "total_rewards_std = []\n",
    "times = [datetime.now()]\n",
    "\n",
    "writer = SummaryWriter(logdir=logdir)\n",
    "\n",
    "# Progress tracking function\n",
    "def progress(num_steps, metrics):\n",
    "    episode_reward = metrics[\"eval/episode_reward\"]\n",
    "    if jp.isnan(episode_reward) or jp.isinf(episode_reward):\n",
    "        print(\"Warning: NaN/Inf reward encountered, aborting.\")\n",
    "        run_duration = str(datetime.now() - times[0])\n",
    "        send_message_sync(\n",
    "            task=\"Cave Exploration RL Training\",\n",
    "            duration=run_duration,\n",
    "            result=\"Failed: NaN/Inf reward encountered\"\n",
    "        )\n",
    "        raise ValueError(f\"NaN/Inf reward encountered at step {num_steps}: {episode_reward}\")\n",
    "    \n",
    "    times.append(datetime.now())\n",
    "    timesteps.append(num_steps)\n",
    "    total_rewards.append(episode_reward)\n",
    "    total_rewards_std.append(metrics[\"eval/episode_reward_std\"])\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    for key, value in metrics.items():\n",
    "        if not (jp.isnan(value) or jp.isinf(value)):\n",
    "            writer.add_scalar(key, value, num_steps)\n",
    "        else:\n",
    "            print(f\"Warning: Skipping NaN/Inf value for metric '{key}' at step {num_steps}\")\n",
    "    \n",
    "    writer.flush()\n",
    "    metrics[\"timesteps\"] = num_steps\n",
    "    metrics[\"time\"] = (times[-1] - times[0]).total_seconds()\n",
    "    rewards.append(metrics)\n",
    "    \n",
    "    percent_complete = (num_steps / ppo_training_params[\"num_timesteps\"]) * 100\n",
    "    if num_steps == 0:\n",
    "        remaining_time_str = \"unknown\"\n",
    "    else:\n",
    "        remaining_time = (ppo_training_params[\"num_timesteps\"] - num_steps) * (times[-1] - times[0]).total_seconds() / num_steps / 60\n",
    "        remaining_time_str = f\"{remaining_time:.2f}\"\n",
    "    \n",
    "    print(f\"step: {num_steps}/{ppo_training_params['num_timesteps']} ({percent_complete:.1f}%), reward: {total_rewards[-1]:.3f} +/- {total_rewards_std[-1]:.3f}, time passed (min): {(times[-1] - times[0]).total_seconds() / 60:.2f} min, calculated time left (min): {remaining_time_str} min\")\n",
    "\n",
    "# Network factory setup\n",
    "network_factory = ppo_networks.make_ppo_networks(observation_size=env.observation_size, action_size=env.action_size)\n",
    "if \"network_factory\" in ppo_params:\n",
    "    if \"network_factory\" in ppo_training_params:\n",
    "        del ppo_training_params[\"network_factory\"]\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "\n",
    "# Checkpoint saving function\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "    del make_policy  # Unused.\n",
    "    orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "    save_args = orbax_utils.save_args_from_target(params)\n",
    "    checkpoint_path = os.path.join(logdir, 'checkpoints')\n",
    "    path = os.path.join(checkpoint_path, f\"{current_step}\")\n",
    "    abs_path = os.path.abspath(path)\n",
    "    orbax_checkpointer.save(abs_path, params, force=True, save_args=save_args)\n",
    "\n",
    "# Save configurations\n",
    "print(\"Saving configs\")\n",
    "configs = {\n",
    "    \"env_cfg\": env_cfg.to_dict(),\n",
    "    \"ppo_params\": ppo_training_params\n",
    "}\n",
    "\n",
    "def replace_infinity(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: replace_infinity(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [replace_infinity(v) for v in obj]\n",
    "    elif isinstance(obj, float) and obj == float('inf'):\n",
    "        return 1e308\n",
    "    return obj\n",
    "\n",
    "configs = replace_infinity(configs)\n",
    "config_path = os.path.join(logdir, 'config.json')\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(configs, fp, indent=4)\n",
    "print(f\"Configuration saved to {config_path}\")\n",
    "writer.add_text('config', json.dumps(configs, indent=4))\n",
    "\n",
    "# Setup training function\n",
    "randomizer = reachbot_randomize\n",
    "train_fn = functools.partial(\n",
    "    ppo.train, \n",
    "    **dict(ppo_training_params),\n",
    "    network_factory=network_factory,\n",
    "    progress_fn=progress,\n",
    "    policy_params_fn=policy_params_fn,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "print(\"Training the model...\")\n",
    "try:\n",
    "    make_inference_fn, params, metrics = train_fn(\n",
    "        environment=env,\n",
    "        wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "    )\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    run_duration = str(datetime.now() - times[0])\n",
    "    send_message_sync(\n",
    "        task=\"Cave Exploration RL Training\",\n",
    "        duration=run_duration,\n",
    "        result=f\"Failed: {e}\"\n",
    "    )\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(f\"time to jit: {times[1] - times[0]}\")\n",
    "print(f\"time to train: {times[-1] - times[1]}\")\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(logdir, 'results.txt')\n",
    "with open(results_path, 'w') as f:\n",
    "    for i in range(len(total_rewards)):\n",
    "        f.write(f\"step: {timesteps[i]}, reward: {total_rewards[i]}, reward_std: {total_rewards_std[i]}\\n\")\n",
    "    f.write(f\"Time to jit: {times[1] - times[0]}\\n\")\n",
    "    f.write(f\"Time to train: {times[-1] - times[1]}\\n\")\n",
    "\n",
    "# Save rewards as JSON\n",
    "def nest_flat_dict(flat_dict):\n",
    "    nested_dict = {}\n",
    "    for key, value in flat_dict.items():\n",
    "        parts = key.split('/')\n",
    "        d = nested_dict\n",
    "        for i, part in enumerate(parts):\n",
    "            is_last_part = (i == len(parts) - 1)\n",
    "            if is_last_part:\n",
    "                if isinstance(d.get(part), dict):\n",
    "                    d[part]['value'] = value\n",
    "                else:\n",
    "                    d[part] = value\n",
    "            else:\n",
    "                if not isinstance(d.get(part), dict):\n",
    "                    d[part] = {'value': d[part]} if part in d else {}\n",
    "                d = d[part]\n",
    "    return nested_dict\n",
    "\n",
    "nested_rewards = [nest_flat_dict(r) for r in rewards]\n",
    "rewards_path = os.path.join(logdir, 'rewards.json')\n",
    "with open(rewards_path, 'w') as fp:\n",
    "    json.dump(nested_rewards, fp, indent=4, cls=JaxArrayEncoder)\n",
    "\n",
    "# Save trained parameters\n",
    "params_path = os.path.join(logdir, 'params')\n",
    "model.save_params(params_path, params)\n",
    "\n",
    "print(f\"Training completed! Results saved to: {logdir}\")\n",
    "print(f\"Final reward: {total_rewards[-1]:.3f} Â± {total_rewards_std[-1]:.3f}\")\n",
    "\n",
    "# Store these variables for the video generation cell\n",
    "trained_params = params\n",
    "trained_make_inference_fn = make_inference_fn\n",
    "trained_env = env\n",
    "trained_logdir = logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c24df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up rollout for video creation...\n",
      "Running rollout for 1 episode(s) with 2000 steps each...\n",
      "Episode 1/1\n",
      "  Step 0/2000\n",
      "  Step 500/2000\n",
      "  Step 1000/2000\n",
      "  Step 1500/2000\n",
      "Rollout completed with 2001 states\n",
      "Rendering video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2001/2001 [00:39<00:00, 51.26it/s]\n",
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "/home/ga53voq/.conda/envs/pyenv/lib/python3.12/subprocess.py:1885: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _fork_exec(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendered 2001 frames\n",
      "Saving video to /home/ga53voq/master_thesis/tasks/cave_exploration/logs/cave_exploration-2025-07-05_18-20-14/posttraining.mp4 at 50.0 FPS...\n",
      "Video saved successfully to /home/ga53voq/master_thesis/tasks/cave_exploration/logs/cave_exploration-2025-07-05_18-20-14/posttraining.mp4\n",
      "\n",
      "=== TRAINING AND VIDEO CREATION COMPLETE ===\n",
      "Log directory: /home/ga53voq/master_thesis/tasks/cave_exploration/logs/cave_exploration-2025-07-05_18-20-14\n",
      "Video file: /home/ga53voq/master_thesis/tasks/cave_exploration/logs/cave_exploration-2025-07-05_18-20-14/posttraining.mp4\n",
      "Training duration: 2:12:23.700416\n",
      "Final result: Final reward: 12.860 Â± 9.270\n"
     ]
    }
   ],
   "source": [
    "# Video creation from trained model\n",
    "# Free up training memory before rendering\n",
    "del train_fn, network_factory, writer\n",
    "gc.collect()\n",
    "\n",
    "# Use the already loaded model and environment from the training cell\n",
    "env = trained_env\n",
    "params = trained_params\n",
    "make_inference_fn = trained_make_inference_fn\n",
    "logdir = trained_logdir\n",
    "\n",
    "# Setup JIT compiled functions for inference\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "inference_fn = make_inference_fn(params, deterministic=True)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "print(\"Setting up rollout for video creation...\")\n",
    "\n",
    "# Rollout parameters\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rollout = []\n",
    "n_episodes = 1\n",
    "rollout_steps = 2000\n",
    "\n",
    "# Set command (if needed for environment)\n",
    "x_vel = 0.2\n",
    "y_vel = 0.2\n",
    "yaw_vel = 0.0\n",
    "command = jp.array([x_vel, y_vel, yaw_vel])\n",
    "\n",
    "# Rollout policy and record simulation\n",
    "print(f\"Running rollout for {n_episodes} episode(s) with {rollout_steps} steps each...\")\n",
    "for episode in range(n_episodes):\n",
    "    print(f\"Episode {episode + 1}/{n_episodes}\")\n",
    "    state = jit_reset(rng)\n",
    "    rollout.append(state)\n",
    "    \n",
    "    for i in range(rollout_steps):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"  Step {i}/{rollout_steps}\")\n",
    "            \n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "        \n",
    "        # Check for numerical issues\n",
    "        if jp.any(jp.isinf(ctrl)) or jp.any(jp.isnan(ctrl)):\n",
    "            print(f\"Numerical issue detected in control at step {i}. Stopping rollout.\")\n",
    "            break\n",
    "            \n",
    "        state = jit_step(state, ctrl)\n",
    "        \n",
    "        # Set command if the environment supports it\n",
    "        if hasattr(state, 'info') and 'command' in state.info:\n",
    "            state.info[\"command\"] = command\n",
    "            \n",
    "        rollout.append(state)\n",
    "\n",
    "print(f\"Rollout completed with {len(rollout)} states\")\n",
    "\n",
    "# Render video\n",
    "print(\"Rendering video...\")\n",
    "render_every = 1  # Render every frame\n",
    "width = 1920      # Full HD width\n",
    "height = 1080     # Full HD height\n",
    "\n",
    "frames = env.render(rollout[::render_every], camera='track_global', width=width, height=height)\n",
    "print(f\"Rendered {len(frames)} frames\")\n",
    "\n",
    "# Save video\n",
    "video_path = os.path.join(logdir, 'posttraining.mp4')\n",
    "fps = 1.0 / env.dt\n",
    "\n",
    "print(f\"Saving video to {video_path} at {fps} FPS...\")\n",
    "imageio.mimsave(video_path, frames, fps=fps)\n",
    "print(f\"Video saved successfully to {video_path}\")\n",
    "\n",
    "# Send completion notification\n",
    "run_duration = str(times[-1] - times[0])\n",
    "if total_rewards:\n",
    "    result = f\"Final reward: {total_rewards[-1]:.3f} Â± {total_rewards_std[-1]:.3f}\"\n",
    "else:\n",
    "    result = \"No rewards recorded.\"\n",
    "\n",
    "send_message_sync(\n",
    "    task=\"Cave Exploration RL Training\",\n",
    "    duration=run_duration,\n",
    "    result=result\n",
    ")\n",
    "\n",
    "print(\"\\n=== TRAINING AND VIDEO CREATION COMPLETE ===\")\n",
    "print(f\"Log directory: {logdir}\")\n",
    "print(f\"Video file: {video_path}\")\n",
    "print(f\"Training duration: {run_duration}\")\n",
    "print(f\"Final result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
