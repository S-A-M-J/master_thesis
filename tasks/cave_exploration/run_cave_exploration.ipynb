{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efbc0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax.io import model\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from mujoco_playground.config import locomotion_params\n",
    "\n",
    "from ml_collections import config_dict\n",
    "\n",
    "ENV_STR = 'Go1JoystickFlatTerrain'\n",
    "\n",
    "ppo_params = locomotion_params.brax_ppo_config(ENV_STR)\n",
    "\n",
    "# Modify params for faster training\n",
    "ppo_params.num_timesteps = 10_000_000  # Reduce from 60000000\n",
    "ppo_params.episode_length = 2000  # Max episode length\n",
    "ppo_params.num_envs = 4096  # Reduce from 2048\n",
    "ppo_params.batch_size = 512  # Number of samples randomly chosen from the rollout data for training\n",
    "ppo_params.num_minibatches = 16  # Splits batch_size into num_minibatches for separate gradient updates\n",
    "ppo_params.num_updates_per_batch = 8  # Reduce from 16\n",
    "ppo_params.unroll_length = 100  # Number of steps to run in each environment before gathering rollouts\n",
    "ppo_params.entropy_cost = 0.02\n",
    "ppo_params.learning_rate = 0.0003  # Learning rate for the optimizerppo\n",
    "ppo_params.network_factory = config_dict.create(\n",
    "    policy_hidden_layer_sizes = (1024,512,256,128),\n",
    "    value_hidden_layer_sizes = (1024,512,256,128),\n",
    "    policy_obs_key = 'state',\n",
    "    value_obs_key = 'privileged_state',\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# For Jupyter notebooks, use os.getcwd() instead of __file__\n",
    "notebook_dir = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(notebook_dir, '../..')))\n",
    "\n",
    "from tasks.cave_exploration.cave_exploration import default_config as reachbot_config\n",
    "\n",
    "env_cfg = reachbot_config()\n",
    "\n",
    "env_cfg.sim_dt = 0.004\n",
    "env_cfg.action_scale = 0.2 # Scale the actions to make them more manageable\n",
    "\n",
    "\n",
    "env_cfg.reward_config.scales.orientation = 0.0 # Disable orientation reward\n",
    "env_cfg.reward_config.scales.lin_vel_z = 0.0 # Disable velocity reward\n",
    "env_cfg.reward_config.scales.ang_vel_xy = 0.0 # Disable xy velocity reward\n",
    "env_cfg.reward_config.scales.feet_slip = 0.0 # Disable z velocity reward\n",
    "env_cfg.reward_config.scales.stand_still = -0.0001 # Disable feet height reward\n",
    "env_cfg.reward_config.scales.torques = -0.0000 # Enable torques reward\n",
    "env_cfg.reward_config.scales.action_rate = 0.0 # Disable LIDAR reward\n",
    "env_cfg.reward_config.scales.energy = 0.0 # Enable\n",
    "env_cfg.reward_config.scales.distance_to_target = 100.0 # Disable LIDAR reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8eddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU device: cuda:0\n",
      "Found 3 cave folders in /home/ga53voq/master_thesis/tasks/cave_exploration/environment/caves.\n",
      "Loading 1 cave environments.\n",
      "initial_qpos (cave_batch_loader): [-0.006  0.057  0.124  0.987 -0.212 -0.008 -0.031  0.016  0.001  0.028\n",
      " -0.016  0.013  0.027 -0.     0.006  0.028 -0.002 -0.     0.028]\n",
      "Floor boxes detected: 5909\n",
      "CaveExplore task initialized with model: ReachbotModelType.BASIC\n",
      "CaveExplore task action space: 12\n",
      "initial_qpos (cave_batch_loader): [-0.006  0.057  0.124  0.987 -0.212 -0.008 -0.031  0.016  0.001  0.028\n",
      " -0.016  0.013  0.027 -0.     0.006  0.028 -0.002 -0.     0.028]\n",
      "Floor boxes detected: 5909\n",
      "CaveExplore task initialized with model: ReachbotModelType.BASIC\n",
      "CaveExplore task action space: 12\n",
      "CaveExplore task observation space: {'privileged_state': (204,), 'state': (145,)}\n",
      "Training parameters:\n",
      "  action_repeat: 1\n",
      "  batch_size: 512\n",
      "  discounting: 0.97\n",
      "  entropy_cost: 0.02\n",
      "  episode_length: 2000\n",
      "  learning_rate: 0.0003\n",
      "  max_grad_norm: 1.0\n",
      "  network_factory: policy_hidden_layer_sizes: &id001 !!python/tuple\n",
      "- 1024\n",
      "- 512\n",
      "- 256\n",
      "- 128\n",
      "policy_obs_key: state\n",
      "value_hidden_layer_sizes: *id001\n",
      "value_obs_key: privileged_state\n",
      "\n",
      "  normalize_observations: True\n",
      "  num_envs: 4096\n",
      "  num_evals: 10\n",
      "  num_minibatches: 16\n",
      "  num_resets_per_eval: 1\n",
      "  num_timesteps: 10000000\n",
      "  num_updates_per_batch: 8\n",
      "  reward_scaling: 1.0\n",
      "  unroll_length: 100\n",
      "CaveExplore task observation space: {'privileged_state': (204,), 'state': (145,)}\n",
      "Training parameters:\n",
      "  action_repeat: 1\n",
      "  batch_size: 512\n",
      "  discounting: 0.97\n",
      "  entropy_cost: 0.02\n",
      "  episode_length: 2000\n",
      "  learning_rate: 0.0003\n",
      "  max_grad_norm: 1.0\n",
      "  network_factory: policy_hidden_layer_sizes: &id001 !!python/tuple\n",
      "- 1024\n",
      "- 512\n",
      "- 256\n",
      "- 128\n",
      "policy_obs_key: state\n",
      "value_hidden_layer_sizes: *id001\n",
      "value_obs_key: privileged_state\n",
      "\n",
      "  normalize_observations: True\n",
      "  num_envs: 4096\n",
      "  num_evals: 10\n",
      "  num_minibatches: 16\n",
      "  num_resets_per_eval: 1\n",
      "  num_timesteps: 10000000\n",
      "  num_updates_per_batch: 8\n",
      "  reward_scaling: 1.0\n",
      "  unroll_length: 100\n",
      "Training the model...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ga53voq/.conda/envs/pyenv/lib/python3.12/site-packages/jax/_src/interpreters/xla.py:119: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0/10000000 (0.0%), reward: nan +/- nan\n"
     ]
    }
   ],
   "source": [
    "# @title Run RL simulation with joystick control\n",
    "\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Tell XLA to use Triton GEMM, this    pip install numpy<2.0 improves steps/sec by ~30% on some GPUs\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "\n",
    "\n",
    "\n",
    "# This helps with nan values being returned from the model while costing some perf. See github @brax for more info\n",
    "# Other fix: \n",
    "#jax.config.update('jax_enable_x64', True) # However, this will slow down the training a lot\n",
    "#jax.config.update('jax_default_matmul_precision', 'high')\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "# Check if GPU is available\n",
    "\n",
    "gpu_available = jax.devices()[0].platform == 'gpu'\n",
    "print(f\"GPU available: {gpu_available}\")\n",
    "\n",
    "# Print GPU details\n",
    "if gpu_available:\n",
    "    gpu_device = jax.devices('gpu')[0]\n",
    "    print(f\"GPU device: {gpu_device}\")\n",
    "else:\n",
    "    print(\"No GPU device found.\")\n",
    "\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Function to handle the interrupt signal\n",
    "def signal_handler(sig, frame):\n",
    "    print('Program exited via keyboard interrupt')\n",
    "    sys.exit(0)\n",
    "\n",
    "# Register the signal handler for SIGINT (Ctrl+C)\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "import mujoco\n",
    "\n",
    "import json\n",
    "# Importing the necessary libraries\n",
    "from datetime import datetime\n",
    "import functools\n",
    "# Run the code on the CPU rather than the GPU\n",
    "# Normally the code runs on the GPU or any other accelerator that is available\n",
    "#os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "\n",
    "import mujoco\n",
    "from flax.training import orbax_utils\n",
    "from orbax import checkpoint as ocp\n",
    "from mujoco_playground.config import locomotion_params\n",
    "\n",
    "from tasks.cave_exploration.cave_exploration import CaveExplore\n",
    "from tasks.common.randomize import domain_randomize as reachbot_randomize\n",
    "from mujoco_playground import registry\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.telegram_messenger import send_message_sync\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Store data from training\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "class JaxArrayEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, jp.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "def save_video(frames, video_path, fps):\n",
    "    import imageio\n",
    "    imageio.mimsave(video_path, frames, fps=fps)\n",
    "       \n",
    "\n",
    "def trainModel():\n",
    "\n",
    "  # Create log directory for training run\n",
    "  datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "  logdir = os.path.join(os.getcwd(), \"logs/cave_exploration-\"+datetime_str)\n",
    "  os.makedirs(logdir, exist_ok=True) \n",
    "  \n",
    "\n",
    "  env = CaveExplore(config=env_cfg, lidar_num_horizontal_rays=20, lidar_max_range=15.0, lidar_horizontal_angle_range=jp.pi * 2, lidar_vertical_angle_range=jp.pi / 6) # Updated LIDAR params for 3D\n",
    "\n",
    "  env_cfg.episode_length = ppo_params[\"episode_length\"]\n",
    "\n",
    "  print(\"Training parameters:\")\n",
    "\n",
    "  for key, value in ppo_params.items():\n",
    "      print(f\"  {key}: {value}\")\n",
    "  timesteps = []\n",
    "  rewards = []\n",
    "  total_rewards = []\n",
    "  total_rewards_std = []\n",
    "\n",
    "  writer = SummaryWriter(logdir=logdir)\n",
    "\n",
    "\n",
    "  # Function to display the training progress\n",
    "  def progress(num_steps, metrics):\n",
    "    times.append(datetime.now())\n",
    "    timesteps.append(num_steps)\n",
    "    total_rewards.append(metrics[\"eval/episode_reward\"])\n",
    "    total_rewards_std.append(metrics[\"eval/episode_reward_std\"])\n",
    "    for key, value in metrics.items():\n",
    "        writer.add_scalar(key, value, num_steps)\n",
    "        writer.flush()\n",
    "    metrics[\"timesteps\"] = num_steps\n",
    "    metrics[\"time\"] = (times[-1] - times[0]).total_seconds()\n",
    "    rewards.append(metrics)\n",
    "    percent_complete = (num_steps / ppo_params[\"num_timesteps\"]) * 100\n",
    "    print(f\"step: {num_steps}/{ppo_params['num_timesteps']} ({percent_complete:.1f}%), reward: {total_rewards[-1]:.3f} +/- {total_rewards_std[-1]:.3f}\")\n",
    "\n",
    "  \n",
    "# Getting the network factory\n",
    "  ppo_training_params = dict(ppo_params)\n",
    "  network_factory = ppo_networks.make_ppo_networks(observation_size=env.observation_size, action_size=env.action_size)\n",
    "  if \"network_factory\" in ppo_params:\n",
    "    if \"network_factory\" in ppo_training_params:\n",
    "      del ppo_training_params[\"network_factory\"]\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "\n",
    "  def policy_params_fn(current_step, make_policy, params):\n",
    "    del make_policy  # Unused.\n",
    "    orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "    save_args = orbax_utils.save_args_from_target(params)\n",
    "    checkpoint_path = os.path.join(logdir, 'checkpoints')\n",
    "    path = os.path.join(checkpoint_path, f\"{current_step}\")\n",
    "    abs_path = os.path.abspath(path)\n",
    "    orbax_checkpointer.save(abs_path, params, force=True, save_args=save_args)\n",
    "\n",
    "  #randomizer = registry.get_domain_randomizer(ENV_STR)\n",
    "  randomizer = reachbot_randomize\n",
    "\n",
    "  # Training the model\n",
    "  print(\"Training the model...\")\n",
    "  train_fn = functools.partial(\n",
    "      ppo.train, **dict(ppo_training_params),\n",
    "      network_factory=network_factory,\n",
    "      progress_fn=progress,\n",
    "      policy_params_fn=policy_params_fn,\n",
    "      #randomization_fn=randomizer,\n",
    "  )\n",
    "\n",
    "  # Function to control the trained agents actions in the environment\n",
    "  # Params: Stores the weights of the trained model\n",
    "  # Metrics: Contains information about the training process such as performance over time\n",
    "  from mujoco_playground import wrapper\n",
    "  # Run training\n",
    "  try:\n",
    "    make_inference_fn, params, metrics = train_fn(\n",
    "        environment=env,\n",
    "        wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "    )\n",
    "\n",
    "  except Exception as e:\n",
    "    import traceback\n",
    "    run_duration = str(datetime.now() - times[0])\n",
    "    # --- Fix for Jupyter: avoid asyncio.run in running event loop ---\n",
    "    import asyncio\n",
    "    from utils.telegram_messenger import send_telegram_message\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        task = loop.create_task(send_telegram_message(\n",
    "            task=\"Cave Exploration RL Training\",\n",
    "            duration=run_duration,\n",
    "            result=f\"Failed: {e}\"\n",
    "        ))\n",
    "    else:\n",
    "        loop.run_until_complete(send_telegram_message(\n",
    "            task=\"Cave Exploration RL Training\",\n",
    "            duration=run_duration,\n",
    "            result=f\"Failed: {e}\"\n",
    "        ))\n",
    "    # --- End fix ---\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "  print(f\"time to jit: {times[1] - times[0]}\")\n",
    "  print(f\"time to train: {times[-1] - times[1]}\")\n",
    "\n",
    "  # Save configs as json in results\n",
    "  configs = {\n",
    "      \"env_cfg\": env_cfg.to_dict(),\n",
    "      \"ppo_params\": ppo_params\n",
    "  }\n",
    "  config_path = os.path.join(logdir, 'config.json')\n",
    "  # Replace Infinity with a large number or null\n",
    "  def replace_infinity(obj):\n",
    "      if isinstance(obj, dict):\n",
    "          return {k: replace_infinity(v) for k, v in obj.items()}\n",
    "      elif isinstance(obj, list):\n",
    "          return [replace_infinity(v) for v in obj]\n",
    "      elif isinstance(obj, float) and obj == float('inf'):\n",
    "          return 1e308\n",
    "      return obj\n",
    "\n",
    "  # Update the configs before saving\n",
    "  configs = replace_infinity(configs)\n",
    "\n",
    "  # Save environment configuration\n",
    "  with open(config_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(configs, fp, indent=4)\n",
    "  print(f\"Configuration saved to {config_path}\")\n",
    "  writer.add_text('config', json.dumps(configs, indent=4))\n",
    "\n",
    "  # Store the results in a file\n",
    "  results_path = os.path.join(logdir, 'results.txt')\n",
    "  with open(results_path, 'w') as f:\n",
    "    for i in range(len(total_rewards)):\n",
    "      f.write(f\"step: {timesteps[i]}, reward: {total_rewards[i]}, reward_std: {total_rewards_std[i]}\\n\")\n",
    "    f.write(f\"Time to jit: {times[1] - times[0]}\\n\")\n",
    "    f.write(f\"Time to train: {times[-1] - times[1]}\\n\")\n",
    "\n",
    "  # Save the rewards as JSON\n",
    "  rewards_path = os.path.join(logdir, 'rewards.json')\n",
    "  def nest_flat_dict(flat_dict):\n",
    "    \"\"\"Converts a dictionary with '/' in keys to a nested dictionary.\"\"\"\n",
    "    nested_dict = {}\n",
    "    for key, value in flat_dict.items():\n",
    "        parts = key.split('/')\n",
    "        d = nested_dict\n",
    "        for i, part in enumerate(parts):\n",
    "            is_last_part = (i == len(parts) - 1)\n",
    "            if is_last_part:\n",
    "                # If a dictionary already exists here, we're setting the 'value' for that group.\n",
    "                if isinstance(d.get(part), dict):\n",
    "                    d[part]['value'] = value\n",
    "                else:\n",
    "                    d[part] = value\n",
    "            else:\n",
    "                # If the path item is not a dict, convert it to one to allow nesting.\n",
    "                if not isinstance(d.get(part), dict):\n",
    "                    d[part] = {'value': d[part]} if part in d else {}\n",
    "                d = d[part]\n",
    "    return nested_dict\n",
    "  \n",
    "  nested_rewards = [nest_flat_dict(r) for r in rewards]\n",
    "  with open(rewards_path, 'w') as fp:\n",
    "      json.dump(nested_rewards, fp, indent=4, cls=JaxArrayEncoder)\n",
    "\n",
    "  \n",
    "  params_path = os.path.join(logdir, 'params')\n",
    "  model.save_params(params_path, params)\n",
    "\n",
    "  jit_reset = jax.jit(env.reset)\n",
    "  jit_step = jax.jit(env.step)\n",
    "  jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n",
    "\n",
    "\n",
    "  rng = jax.random.PRNGKey(0)\n",
    "  rollout = []\n",
    "  n_episodes = 1\n",
    "\n",
    "  # Rollout policy and record simulation\n",
    "  for _ in range(n_episodes):\n",
    "    state = jit_reset(rng)\n",
    "    rollout.append(state)\n",
    "    for i in range(1200):\n",
    "      act_rng, rng = jax.random.split(rng)\n",
    "      ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "      state = jit_step(state, ctrl)\n",
    "      rollout.append(state)\n",
    "\n",
    "  render_every = 1\n",
    "  frames = env.render(rollout[::render_every], camera='track_global', width=1920, height=1080)\n",
    "  video_path = os.path.join(logdir, 'posttraining.mp4')\n",
    "  fps = 1.0 / env.dt\n",
    "  ctx = mp.get_context(\"spawn\")\n",
    "  p = ctx.Process(target=save_video, args=(frames, video_path, fps))\n",
    "  p.start()\n",
    "  p.join()\n",
    "\n",
    "  # Calculate run duration\n",
    "  run_duration = str(times[-1] - times[0])\n",
    "  # Get final reward and std if available\n",
    "  if total_rewards:\n",
    "      result = f\"Final reward: {total_rewards[-1]:.3f} Â± {total_rewards_std[-1]:.3f}\"\n",
    "  else:\n",
    "      result = \"No rewards recorded.\"\n",
    "  # --- Fix for Jupyter: avoid asyncio.run in running event loop ---\n",
    "  import asyncio\n",
    "  from utils.telegram_messenger import send_telegram_message\n",
    "  loop = asyncio.get_event_loop()\n",
    "  if loop.is_running():\n",
    "      task = loop.create_task(send_telegram_message(\n",
    "          task=\"Cave Exploration RL Training\",\n",
    "          duration=run_duration,\n",
    "          result=result\n",
    "      ))\n",
    "  else:\n",
    "      loop.run_until_complete(send_telegram_message(\n",
    "          task=\"Cave Exploration RL Training\",\n",
    "          duration=run_duration,\n",
    "          result=result\n",
    "      ))\n",
    "  # --- End fix ---\n",
    "\n",
    "trainModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
